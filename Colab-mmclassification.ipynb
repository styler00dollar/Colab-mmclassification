{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Colab-mmclassification.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdFDrB-monSW"
      },
      "source": [
        "# Colab-mmclassification\n",
        "\n",
        "Original repo: [open-mmlab/mmclassification](https://github.com/open-mmlab/mmclassification)\n",
        "\n",
        "My fork: [styler00dollar/Colab-mmclassification](https://github.com/styler00dollar/Colab-mmclassification)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7-hLPm2SqHc"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "BfWOn7AgS46a"
      },
      "source": [
        "#@title install\n",
        "!git clone https://github.com/open-mmlab/mmclassification.git\n",
        "%cd mmclassification\n",
        "!pip install -e .  # or \"python setup.py develop\"\n",
        "!pip install mmcv-full -f https://download.openmmlab.com/mmcv/dist/cu101/torch1.7.0/index.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1Cgjc3PRYoOz"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "print('Google Drive connected.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUlaZMOxYt7D",
        "cellView": "form"
      },
      "source": [
        "#@title copy and extract own dataset\n",
        "!cp \"/content/drive/MyDrive/dataset.7z\" \"/content/dataset.7z\"\n",
        "%cd /content/\n",
        "!7z x /content/dataset.7z"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y5FB7-tTT4J"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cK7Ph6A2HNs"
      },
      "source": [
        "Spaces are very bad, you need to remove them. A simple fix is md5."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "yzcaXAb6zF8n"
      },
      "source": [
        "#@title (Optional) Hash all files to md5 and move broken files to another folder\n",
        "import glob\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "input_folder = '/content/images/' #@param {type:\"string\"}\n",
        "broken_folder = '/content/broken/' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "hash_md5 = hashlib.md5()\n",
        "for subdir, dirs, files in os.walk(input_folder):\n",
        "  for dir in dirs:\n",
        "      files = glob.glob(input_folder + \"/\" + dir + '/**/*.png', recursive=True)\n",
        "      files_jpg = glob.glob(input_folder + \"/\" + dir + '/**/*.jpg', recursive=True)\n",
        "      files.extend(files_jpg)\n",
        "\n",
        "      for f in tqdm(files):\n",
        "        image = cv2.imread(f)\n",
        "        \n",
        "        original_folder = os.path.split(f)[0]\n",
        "        with open(f, \"rb\") as file:\n",
        "            for chunk in iter(lambda: file.read(4096), b\"\"):\n",
        "                hash_md5.update(chunk)\n",
        "        if image is not None:\n",
        "            shutil.move(f, os.path.join(original_folder, os.path.basename(hash_md5.hexdigest()+os.path.splitext(f)[1])))\n",
        "        else:\n",
        "            shutil.move(f, os.path.join(broken_folder, os.path.basename(hash_md5.hexdigest()+os.path.splitext(f)[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oiqa4MbUQSSI",
        "cellView": "form"
      },
      "source": [
        "#@title (Optional) Re-saving png images with OpenCV to avoid ``libpng warning: iCCP: known incorrect sRGB profile``\n",
        "import glob\n",
        "import hashlib\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "import shutil\n",
        "input_folder = '/content/images/' #@param {type:\"string\"}\n",
        "broken_folder = '/content/broken/' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "hash_md5 = hashlib.md5()\n",
        "for subdir, dirs, files in os.walk(input_folder):\n",
        "  for dir in dirs:\n",
        "      files = glob.glob(input_folder + \"/\" + dir + '/**/*.png', recursive=True)\n",
        "\n",
        "      for f in tqdm(files):\n",
        "        image = cv2.imread(f)\n",
        "        if image is not None:\n",
        "            cv2.imwrite(f, image)\n",
        "        else:\n",
        "            with open(f, \"rb\") as file:\n",
        "              for chunk in iter(lambda: file.read(4096), b\"\"):\n",
        "                  hash_md5.update(chunk)\n",
        "            shutil.move(f, os.path.join(broken_folder, os.path.basename(hash_md5.hexdigest()+os.path.splitext(f)[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdHjgyjFebkR",
        "cellView": "form"
      },
      "source": [
        "#@title [dataset creation](https://github.com/bentrevett/pytorch-image-classification/blob/master/5_resnet.ipynb) (Split dataset in ```/train``` and ```/test```. Searches for ```/images```)\n",
        "import os\n",
        "import shutil\n",
        "from tqdm import tqdm\n",
        "%cd /content/\n",
        "TRAIN_RATIO = 0.9 #@param {type:\"number\"}\n",
        "data_dir = '/content/' #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "images_dir = os.path.join(data_dir, 'images')\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "if os.path.exists(train_dir):\n",
        "    shutil.rmtree(train_dir) \n",
        "if os.path.exists(test_dir):\n",
        "    shutil.rmtree(test_dir)\n",
        "    \n",
        "os.makedirs(train_dir)\n",
        "os.makedirs(test_dir)\n",
        "\n",
        "classes = os.listdir(images_dir)\n",
        "\n",
        "for c in classes:\n",
        "    \n",
        "    class_dir = os.path.join(images_dir, c)\n",
        "    \n",
        "    images = os.listdir(class_dir)\n",
        "       \n",
        "    n_train = int(len(images) * TRAIN_RATIO)\n",
        "    \n",
        "    train_images = images[:n_train]\n",
        "    test_images = images[n_train:]\n",
        "    \n",
        "    os.makedirs(os.path.join(train_dir, c), exist_ok = True)\n",
        "    os.makedirs(os.path.join(test_dir, c), exist_ok = True)\n",
        "    \n",
        "    for image in tqdm(train_images):\n",
        "        image_src = os.path.join(class_dir, image)\n",
        "        image_dst = os.path.join(train_dir, c, image) \n",
        "        shutil.copyfile(image_src, image_dst)\n",
        "        \n",
        "    for image in tqdm(test_images):\n",
        "        image_src = os.path.join(class_dir, image)\n",
        "        image_dst = os.path.join(test_dir, c, image) \n",
        "        shutil.copyfile(image_src, image_dst)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yb_38biflgWw",
        "cellView": "form"
      },
      "source": [
        "#@title show amount of files in a certain path\r\n",
        "!ls /path/ | wc -l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXLTkQ-Jk6SZ"
      },
      "source": [
        "Do not skip this. If you don't do that, then the model won't learn and just do thing. Print **your** means and stds and add these values to the config."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "R35Ud-rwWBcj"
      },
      "source": [
        "#@title print means and stds\r\n",
        "import torch\r\n",
        "import torchvision.transforms as transforms\r\n",
        "import torchvision.datasets as datasets\r\n",
        "from tqdm import tqdm\r\n",
        "train_dir = '/content/train' #@param {type:\"string\"}\r\n",
        "train_data = datasets.ImageFolder(root = train_dir, \r\n",
        "                                  transform = transforms.ToTensor())\r\n",
        "\r\n",
        "means = torch.zeros(3)\r\n",
        "stds = torch.zeros(3)\r\n",
        "\r\n",
        "for img, label in tqdm(train_data):\r\n",
        "    means += torch.mean(img, dim = (1,2))\r\n",
        "    stds += torch.std(img, dim = (1,2))\r\n",
        "\r\n",
        "means /= len(train_data)\r\n",
        "stds /= len(train_data)\r\n",
        "\r\n",
        "means = means*255\r\n",
        "stds = stds*255\r\n",
        "\r\n",
        "print(f'Calculated means: {means}')\r\n",
        "print(f'Calculated stds: {stds}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWCCa4h_mdUB"
      },
      "source": [
        "It is needed to create annotation files. Warning: The classes are represented with numbers. Do that once for validation and training data. ```classes.txt``` will show the mapping."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3Ndy_6yksCq",
        "cellView": "form"
      },
      "source": [
        "#@title Generate [needed annotation files](https://github.com/open-mmlab/mmclassification/blob/master/docs/tutorials/new_dataset.md).\n",
        "import os\n",
        "import glob\n",
        "data_dir = '/content/train/' #@param {type:\"string\"}\n",
        "annotation_output = '/content/train.txt' #@param {type:\"string\"}\n",
        "class_output = '/content/classes_train.txt' #@param {type:\"string\"}\n",
        "counter = 0\n",
        "\n",
        "if os.path.exists(annotation_output):\n",
        "  os.remove(annotation_output)\n",
        "if os.path.exists(class_output):\n",
        "  os.remove(class_output)\n",
        "\n",
        "for subdir, dirs, files in os.walk(data_dir):\n",
        "  for dir in dirs:\n",
        "    folder_path = os.path.join(data_dir, dir)\n",
        "\n",
        "    files = glob.glob(folder_path + '/**/*.png', recursive=True)\n",
        "    files_jpg = glob.glob(folder_path + '/**/*.jpg', recursive=True)\n",
        "    files.extend(files_jpg)\n",
        "\n",
        "    f=open(annotation_output,'a')\n",
        "    for ele in files:\n",
        "        f.write(ele+\" \"+str(counter)+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    f=open(class_output,'a')\n",
        "    f.write(str(dir)+\" \"+str(counter)+'\\n')\n",
        "\n",
        "    f.close()\n",
        "\n",
        "    counter += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8aYZn0epA3C"
      },
      "source": [
        "# Example with [mmclassification/configs/resnext/resnext50_32x4d_b32x8_imagenet.py](https://github.com/open-mmlab/mmclassification/blob/master/configs/resnext/resnext50_32x4d_b32x8_imagenet.py).\n",
        "```\n",
        "_base_ = [\n",
        "    '../_base_/models/resnext50_32x4d.py',\n",
        "    '../_base_/datasets/imagenet_bs32.py',\n",
        "    '../_base_/schedules/imagenet_bs256.py', '../_base_/default_runtime.py'\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCgK59n5Z6E3",
        "cellView": "form"
      },
      "source": [
        "#@title imagenet_bs256.py (max epoch)\n",
        "%%writefile /content/mmclassification/configs/_base_/schedules/imagenet_bs256.py\n",
        "# optimizer\n",
        "optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "optimizer_config = dict(grad_clip=None)\n",
        "# learning policy\n",
        "lr_config = dict(policy='step', step=[30, 60, 90])\n",
        "runner = dict(type='EpochBasedRunner', max_epochs=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3odF1gW-ZiB5",
        "cellView": "form"
      },
      "source": [
        "#@title resnext50_32x4d.py (amount classes / topk)\n",
        "%%writefile /content/mmclassification/configs/_base_/models/resnext50_32x4d.py\n",
        "# model settings\n",
        "model = dict(\n",
        "    type='ImageClassifier',\n",
        "    backbone=dict(\n",
        "        type='ResNeXt',\n",
        "        depth=50,\n",
        "        num_stages=4,\n",
        "        out_indices=(3, ),\n",
        "        groups=32,\n",
        "        width_per_group=4,\n",
        "        style='pytorch'),\n",
        "    neck=dict(type='GlobalAveragePooling'),\n",
        "    head=dict(\n",
        "        type='LinearClsHead',\n",
        "        num_classes=1000,\n",
        "        in_channels=2048,\n",
        "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
        "        topk=(1, 1),\n",
        "    ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IO2eFgy6UfR5",
        "cellView": "form"
      },
      "source": [
        "#@title imagenet_bs32.py (edit paths)\n",
        "%%writefile /content/mmclassification/configs/_base_/datasets/imagenet_bs32.py\n",
        "# dataset settings\n",
        "dataset_type = 'ImageNet'\n",
        "img_norm_cfg = dict(\n",
        "    mean=[123.675, 116.28, 103.53], std=[58.395, 57.12, 57.375], to_rgb=True)\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(type='RandomResizedCrop', size=224),\n",
        "    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),\n",
        "    dict(type='Normalize', **img_norm_cfg),\n",
        "    dict(type='ImageToTensor', keys=['img']),\n",
        "    dict(type='ToTensor', keys=['gt_label']),\n",
        "    dict(type='Collect', keys=['img', 'gt_label'])\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(type='Resize', size=(256, -1)),\n",
        "    dict(type='CenterCrop', crop_size=224),\n",
        "    dict(type='Normalize', **img_norm_cfg),\n",
        "    dict(type='ImageToTensor', keys=['img']),\n",
        "    dict(type='Collect', keys=['img'])\n",
        "]\n",
        "data = dict(\n",
        "    samples_per_gpu=32,\n",
        "    workers_per_gpu=2,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        data_prefix='/content/images',\n",
        "        pipeline=train_pipeline),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        data_prefix='/content/images',\n",
        "        ann_file='/content/test.txt',\n",
        "        pipeline=test_pipeline),\n",
        "    test=dict(\n",
        "        # replace `data/val` with `data/test` for standard test\n",
        "        type=dataset_type,\n",
        "        data_prefix='data/imagenet/val',\n",
        "        ann_file='/content/test.txt',\n",
        "        pipeline=test_pipeline))\n",
        "evaluation = dict(interval=1, metric='accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT_Wgt9pTUwR",
        "cellView": "form"
      },
      "source": [
        "#@title train (resnext50_32x4d_b32x8_imagenet.py)\n",
        "%cd /content/mmclassification/\n",
        "!python tools/train.py /content/mmclassification/configs/resnext/resnext50_32x4d_b32x8_imagenet.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgwG09VUsgSP"
      },
      "source": [
        "# Example with [mmclassification/configs/_base_/models/resnest50.py](https://github.com/open-mmlab/mmclassification/blob/24fd4fb62734cc87c0fec551be9185668c30c52f/configs/_base_/models/resnest50.py). \n",
        "ResNeSt is currently not in the documentation, but can be added manually."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2gRNVOvs65w",
        "cellView": "form"
      },
      "source": [
        "#@title create resnest50.py\n",
        "%%writefile /content/mmclassification/configs/resnest50.py\n",
        "_base_ = [\n",
        "    '/content/mmclassification/configs/_base_/models/resnest50.py',\n",
        "    '/content/mmclassification/configs/_base_/datasets/imagenet_bs32.py',\n",
        "    '/content/mmclassification/configs/_base_/schedules/imagenet_bs256.py', '/content/mmclassification/configs/_base_/default_runtime.py'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_Kxf5WwpSzd",
        "cellView": "form"
      },
      "source": [
        "#@title imagenet.py (adding webp support)\r\n",
        "%%writefile /content/mmclassification/mmcls/datasets/imagenet.py\r\n",
        "import os\r\n",
        "\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "from .base_dataset import BaseDataset\r\n",
        "from .builder import DATASETS\r\n",
        "\r\n",
        "\r\n",
        "def has_file_allowed_extension(filename, extensions):\r\n",
        "    \"\"\"Checks if a file is an allowed extension.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        filename (string): path to a file\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        bool: True if the filename ends with a known image extension\r\n",
        "    \"\"\"\r\n",
        "    filename_lower = filename.lower()\r\n",
        "    return any(filename_lower.endswith(ext) for ext in extensions)\r\n",
        "\r\n",
        "\r\n",
        "def find_folders(root):\r\n",
        "    \"\"\"Find classes by folders under a root.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (string): root directory of folders\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        folder_to_idx (dict): the map from folder name to class idx\r\n",
        "    \"\"\"\r\n",
        "    folders = [\r\n",
        "        d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))\r\n",
        "    ]\r\n",
        "    folders.sort()\r\n",
        "    folder_to_idx = {folders[i]: i for i in range(len(folders))}\r\n",
        "    return folder_to_idx\r\n",
        "\r\n",
        "\r\n",
        "def get_samples(root, folder_to_idx, extensions):\r\n",
        "    \"\"\"Make dataset by walking all images under a root.\r\n",
        "\r\n",
        "    Args:\r\n",
        "        root (string): root directory of folders\r\n",
        "        folder_to_idx (dict): the map from class name to class idx\r\n",
        "        extensions (tuple): allowed extensions\r\n",
        "\r\n",
        "    Returns:\r\n",
        "        samples (list): a list of tuple where each element is (image, label)\r\n",
        "    \"\"\"\r\n",
        "    samples = []\r\n",
        "    root = os.path.expanduser(root)\r\n",
        "    for folder_name in sorted(os.listdir(root)):\r\n",
        "        _dir = os.path.join(root, folder_name)\r\n",
        "        if not os.path.isdir(_dir):\r\n",
        "            continue\r\n",
        "\r\n",
        "        for _, _, fns in sorted(os.walk(_dir)):\r\n",
        "            for fn in sorted(fns):\r\n",
        "                if has_file_allowed_extension(fn, extensions):\r\n",
        "                    path = os.path.join(folder_name, fn)\r\n",
        "                    item = (path, folder_to_idx[folder_name])\r\n",
        "                    samples.append(item)\r\n",
        "    return samples\r\n",
        "\r\n",
        "\r\n",
        "@DATASETS.register_module()\r\n",
        "class ImageNet(BaseDataset):\r\n",
        "    \"\"\"`ImageNet <http://www.image-net.org>`_ Dataset.\r\n",
        "\r\n",
        "    This implementation is modified from\r\n",
        "    https://github.com/pytorch/vision/blob/master/torchvision/datasets/imagenet.py  # noqa: E501\r\n",
        "    \"\"\"\r\n",
        "\r\n",
        "    IMG_EXTENSIONS = ('.jpg', '.jpeg', '.png', '.ppm', '.bmp', '.pgm', '.tif', '.webp')\r\n",
        "    CLASSES = [\r\n",
        "        'tench, Tinca tinca',\r\n",
        "        'goldfish, Carassius auratus',\r\n",
        "        'toilet tissue, toilet paper, bathroom tissue'\r\n",
        "    ]\r\n",
        "\r\n",
        "    def load_annotations(self):\r\n",
        "        if self.ann_file is None:\r\n",
        "            folder_to_idx = find_folders(self.data_prefix)\r\n",
        "            samples = get_samples(\r\n",
        "                self.data_prefix,\r\n",
        "                folder_to_idx,\r\n",
        "                extensions=self.IMG_EXTENSIONS)\r\n",
        "            if len(samples) == 0:\r\n",
        "                raise (RuntimeError('Found 0 files in subfolders of: '\r\n",
        "                                    f'{self.data_prefix}. '\r\n",
        "                                    'Supported extensions are: '\r\n",
        "                                    f'{\",\".join(self.IMG_EXTENSIONS)}'))\r\n",
        "\r\n",
        "            self.folder_to_idx = folder_to_idx\r\n",
        "        elif isinstance(self.ann_file, str):\r\n",
        "            with open(self.ann_file) as f:\r\n",
        "                samples = [x.strip().split(' ') for x in f.readlines()]\r\n",
        "        else:\r\n",
        "            raise TypeError('ann_file must be a str or None')\r\n",
        "        self.samples = samples\r\n",
        "\r\n",
        "        data_infos = []\r\n",
        "        for filename, gt_label in self.samples:\r\n",
        "            info = {'img_prefix': self.data_prefix}\r\n",
        "            info['img_info'] = {'filename': filename}\r\n",
        "            info['gt_label'] = np.array(gt_label, dtype=np.int64)\r\n",
        "            data_infos.append(info)\r\n",
        "        return data_infos"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FOavQSKvooro",
        "cellView": "form"
      },
      "source": [
        "#@title default_runtime.py (setting pretrain to resnest50)\r\n",
        "%%writefile /content/mmclassification/configs/_base_/default_runtime.py\r\n",
        "# checkpoint saving\r\n",
        "checkpoint_config = dict(interval=1)\r\n",
        "# yapf:disable\r\n",
        "log_config = dict(\r\n",
        "    interval=100,\r\n",
        "    hooks=[\r\n",
        "        dict(type='TextLoggerHook'),\r\n",
        "        # dict(type='TensorboardLoggerHook')\r\n",
        "    ])\r\n",
        "# yapf:enable\r\n",
        "dist_params = dict(backend='nccl')\r\n",
        "log_level = 'INFO'\r\n",
        "load_from = '/content/epoch_461.pth'\r\n",
        "resume_from = None\r\n",
        "workflow = [('train', 1)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVR1mI-TYrvt",
        "cellView": "form"
      },
      "source": [
        "#@title imagenet_bs256.py (max epoch)\n",
        "%%writefile /content/mmclassification/configs/_base_/schedules/imagenet_bs256.py\n",
        "# optimizer\n",
        "optimizer = dict(type='SGD', lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "optimizer_config = dict(grad_clip=None)\n",
        "# learning policy\n",
        "lr_config = dict(policy='step', step=[30, 60, 90])\n",
        "runner = dict(type='EpochBasedRunner', max_epochs=2000)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23aLtpiZY5rL",
        "cellView": "form"
      },
      "source": [
        "#@title resnest50.py (amount classes / topk)\n",
        "%%writefile /content/mmclassification/configs/_base_/models/resnest50.py\n",
        "# model settings\n",
        "model = dict(\n",
        "    type='ImageClassifier',\n",
        "    backbone=dict(\n",
        "        type='ResNeSt',\n",
        "        depth=50,\n",
        "        num_stages=4,\n",
        "        out_indices=(3, ),\n",
        "        style='pytorch'),\n",
        "    neck=dict(type='GlobalAveragePooling'),\n",
        "    head=dict(\n",
        "        type='LinearClsHead',\n",
        "        num_classes=2,\n",
        "        in_channels=2048,\n",
        "        loss=dict(type='CrossEntropyLoss', loss_weight=1.0),\n",
        "        topk=(1, 1),\n",
        "    ))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2v3-YZRgtoCQ",
        "cellView": "form"
      },
      "source": [
        "#@title imagenet_bs32.py (edit paths, mean/std)\n",
        "%%writefile /content/mmclassification/configs/_base_/datasets/imagenet_bs32.py\n",
        "# dataset settings\n",
        "dataset_type = 'ImageNet'\n",
        "img_norm_cfg = dict(\n",
        "    mean=[174.1395, 154.785, 154.326], std=[61.6335, 61.4295, 58.8795], to_rgb=True)\n",
        "train_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(type='RandomResizedCrop', size=224),\n",
        "    dict(type='RandomFlip', flip_prob=0.5, direction='horizontal'),\n",
        "    dict(type='Normalize', **img_norm_cfg),\n",
        "    dict(type='ImageToTensor', keys=['img']),\n",
        "    dict(type='ToTensor', keys=['gt_label']),\n",
        "    dict(type='Collect', keys=['img', 'gt_label'])\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(type='LoadImageFromFile'),\n",
        "    dict(type='Resize', size=(256, -1)),\n",
        "    dict(type='CenterCrop', crop_size=224),\n",
        "    dict(type='Normalize', **img_norm_cfg),\n",
        "    dict(type='ImageToTensor', keys=['img']),\n",
        "    dict(type='Collect', keys=['img'])\n",
        "]\n",
        "data = dict(\n",
        "    samples_per_gpu=32,\n",
        "    workers_per_gpu=2,\n",
        "    train=dict(\n",
        "        type=dataset_type,\n",
        "        data_prefix='/content/train/',\n",
        "        pipeline=train_pipeline),\n",
        "    val=dict(\n",
        "        type=dataset_type,\n",
        "        data_prefix='/content/validation/',\n",
        "        ann_file='/content/validation.txt',\n",
        "        pipeline=test_pipeline),\n",
        "    test=dict(\n",
        "        # replace `data/val` with `data/test` for standard test\n",
        "        type=dataset_type,\n",
        "        data_prefix='/content/testing/',\n",
        "        ann_file='/content/testing.txt',\n",
        "        pipeline=test_pipeline))\n",
        "evaluation = dict(interval=1, metric='accuracy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wy29tPyTOMjO",
        "cellView": "form"
      },
      "source": [
        "#@title formatting.py (avoiding img_metas to avoid errors during validation/interference)\n",
        "%%writefile /content/mmclassification/mmcls/datasets/pipelines/formating.py\n",
        "from collections.abc import Sequence\n",
        "\n",
        "import mmcv\n",
        "import numpy as np\n",
        "import torch\n",
        "from mmcv.parallel import DataContainer as DC\n",
        "from PIL import Image\n",
        "\n",
        "from ..builder import PIPELINES\n",
        "\n",
        "\n",
        "def to_tensor(data):\n",
        "    \"\"\"Convert objects of various python types to :obj:`torch.Tensor`.\n",
        "\n",
        "    Supported types are: :class:`numpy.ndarray`, :class:`torch.Tensor`,\n",
        "    :class:`Sequence`, :class:`int` and :class:`float`.\n",
        "    \"\"\"\n",
        "    if isinstance(data, torch.Tensor):\n",
        "        return data\n",
        "    elif isinstance(data, np.ndarray):\n",
        "        return torch.from_numpy(data)\n",
        "    elif isinstance(data, Sequence) and not mmcv.is_str(data):\n",
        "        return torch.tensor(data)\n",
        "    elif isinstance(data, int):\n",
        "        return torch.LongTensor([data])\n",
        "    elif isinstance(data, float):\n",
        "        return torch.FloatTensor([data])\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f'Type {type(data)} cannot be converted to tensor.'\n",
        "            'Supported types are: `numpy.ndarray`, `torch.Tensor`, '\n",
        "            '`Sequence`, `int` and `float`')\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class ToTensor(object):\n",
        "\n",
        "    def __init__(self, keys):\n",
        "        self.keys = keys\n",
        "\n",
        "    def __call__(self, results):\n",
        "        for key in self.keys:\n",
        "            results[key] = to_tensor(results[key])\n",
        "        return results\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(keys={self.keys})'\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class ImageToTensor(object):\n",
        "\n",
        "    def __init__(self, keys):\n",
        "        self.keys = keys\n",
        "\n",
        "    def __call__(self, results):\n",
        "        for key in self.keys:\n",
        "            img = results[key]\n",
        "            if len(img.shape) < 3:\n",
        "                img = np.expand_dims(img, -1)\n",
        "            results[key] = to_tensor(img.transpose(2, 0, 1))\n",
        "        return results\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + f'(keys={self.keys})'\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class Transpose(object):\n",
        "\n",
        "    def __init__(self, keys, order):\n",
        "        self.keys = keys\n",
        "        self.order = order\n",
        "\n",
        "    def __call__(self, results):\n",
        "        for key in self.keys:\n",
        "            results[key] = results[key].transpose(self.order)\n",
        "        return results\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \\\n",
        "            f'(keys={self.keys}, order={self.order})'\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class ToPIL(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, results):\n",
        "        results['img'] = Image.fromarray(results['img'])\n",
        "        return results\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class ToNumpy(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, results):\n",
        "        results['img'] = np.array(results['img'], dtype=np.float32)\n",
        "        return results\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class Collect(object):\n",
        "    \"\"\"\n",
        "    Collect data from the loader relevant to the specific task.\n",
        "\n",
        "    This is usually the last stage of the data loader pipeline. Typically keys\n",
        "    is set to some subset of \"img\" and \"gt_label\".\n",
        "\n",
        "    Args:\n",
        "        keys (Sequence[str]): Keys of results to be collected in ``data``.\n",
        "        meta_keys (Sequence[str], optional): Meta keys to be converted to\n",
        "            ``mmcv.DataContainer`` and collected in ``data[img_metas]``.\n",
        "            Default: ``('filename', 'ori_shape', 'img_shape', 'flip',\n",
        "            'flip_direction', 'img_norm_cfg')``\n",
        "\n",
        "    Returns:\n",
        "        dict: The result dict contains the following keys\n",
        "                - keys in``self.keys``\n",
        "                - ``img_metas`` if avaliable\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 keys,\n",
        "                 meta_keys=('filename', 'ori_shape', 'img_shape', 'flip',\n",
        "                            'flip_direction', 'img_norm_cfg')):\n",
        "        self.keys = keys\n",
        "        self.meta_keys = meta_keys\n",
        "\n",
        "    def __call__(self, results):\n",
        "        data = {}\n",
        "        img_meta = {}\n",
        "        for key in self.meta_keys:\n",
        "            if key in results:\n",
        "                img_meta[key] = results[key]\n",
        "        data['img_metas'] = DC(img_meta, cpu_only=True)\n",
        "        for key in self.keys:\n",
        "            data[key] = results[key]\n",
        "        return data\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__class__.__name__ + \\\n",
        "            f'(keys={self.keys}, meta_keys={self.meta_keys})'\n",
        "\n",
        "\n",
        "@PIPELINES.register_module()\n",
        "class WrapFieldsToLists(object):\n",
        "    \"\"\"Wrap fields of the data dictionary into lists for evaluation.\n",
        "\n",
        "    This class can be used as a last step of a test or validation\n",
        "    pipeline for single image evaluation or inference.\n",
        "\n",
        "    Example:\n",
        "        >>> test_pipeline = [\n",
        "        >>>    dict(type='LoadImageFromFile'),\n",
        "        >>>    dict(type='Normalize',\n",
        "                    mean=[123.675, 116.28, 103.53],\n",
        "                    std=[58.395, 57.12, 57.375],\n",
        "                    to_rgb=True),\n",
        "        >>>    dict(type='ImageToTensor', keys=['img']),\n",
        "        >>>    dict(type='Collect', keys=['img']),\n",
        "        >>>    dict(type='WrapIntoLists')\n",
        "        >>> ]\n",
        "    \"\"\"\n",
        "\n",
        "    def __call__(self, results):\n",
        "        # Wrap dict fields into lists\n",
        "        for key, val in results.items():\n",
        "            results[key] = [val]\n",
        "        return results\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'{self.__class__.__name__}()'\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tu9fLYSUaUwZ",
        "cellView": "form"
      },
      "source": [
        "#@title (Optional) accuracy.py (forcing top1 instead of topk, only do this if you have less than 5 classes)\n",
        "%%writefile /content/mmclassification/mmcls/models/losses/accuracy.py\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def accuracy_numpy(pred, target, topk=1, thrs=None):\n",
        "    if thrs is None:\n",
        "        thrs = 0.0\n",
        "    if isinstance(thrs, float):\n",
        "        thrs = (thrs, )\n",
        "        res_single = True\n",
        "    elif isinstance(thrs, tuple):\n",
        "        res_single = False\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f'thrs should be float or tuple, but got {type(thrs)}.')\n",
        "\n",
        "    res = []\n",
        "    maxk = max(topk)\n",
        "    num = pred.shape[0]\n",
        "    pred_label = pred.argsort(axis=1)[:, -maxk:][:, ::-1]\n",
        "    pred_score = np.sort(pred, axis=1)[:, -maxk:][:, ::-1]\n",
        "\n",
        "    for k in topk:\n",
        "        correct_k = pred_label[:, :k] == target.reshape(-1, 1)\n",
        "        res_thr = []\n",
        "        for thr in thrs:\n",
        "            # Only prediction values larger than thr are counted as correct\n",
        "            _correct_k = correct_k & (pred_score[:, :k] > thr)\n",
        "            _correct_k = np.logical_or.reduce(_correct_k, axis=1)\n",
        "            res_thr.append(_correct_k.sum() * 100. / num)\n",
        "        if res_single:\n",
        "            res.append(res_thr[0])\n",
        "        else:\n",
        "            res.append(res_thr)\n",
        "    return res\n",
        "\n",
        "\n",
        "def accuracy_torch(pred, target, topk=1, thrs=None):\n",
        "    if thrs is None:\n",
        "        thrs = 0.0\n",
        "    if isinstance(thrs, float):\n",
        "        thrs = (thrs, )\n",
        "        res_single = True\n",
        "    elif isinstance(thrs, tuple):\n",
        "        res_single = False\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f'thrs should be float or tuple, but got {type(thrs)}.')\n",
        "\n",
        "    res = []\n",
        "    maxk = max(topk)\n",
        "    num = pred.size(0)\n",
        "    pred_score, pred_label = pred.topk(1, dim=1)\n",
        "    pred_label = pred_label.t()\n",
        "    correct = pred_label.eq(target.view(1, -1).expand_as(pred_label))\n",
        "    for k in topk:\n",
        "        res_thr = []\n",
        "        for thr in thrs:\n",
        "            # Only prediction values larger than thr are counted as correct\n",
        "            _correct = correct & (pred_score.t() > thr)\n",
        "            correct_k = _correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
        "            res_thr.append(correct_k.mul_(100. / num))\n",
        "        if res_single:\n",
        "            res.append(res_thr[0])\n",
        "        else:\n",
        "            res.append(res_thr)\n",
        "    return res\n",
        "\n",
        "\n",
        "def accuracy(pred, target, topk=1, thrs=None):\n",
        "    \"\"\"Calculate accuracy according to the prediction and target\n",
        "\n",
        "    Args:\n",
        "        pred (torch.Tensor | np.array): The model prediction.\n",
        "        target (torch.Tensor | np.array): The target of each prediction\n",
        "        topk (int | tuple[int]): If the predictions in ``topk``\n",
        "            matches the target, the predictions will be regarded as\n",
        "            correct ones. Defaults to 1.\n",
        "        thrs (float, optional): thrs (float | tuple[float], optional):\n",
        "            Predictions with scores under the thresholds are considered\n",
        "            negative. Default to None.\n",
        "\n",
        "    Returns:\n",
        "        float | list[float] | list[list[float]]: If the input ``topk`` is a\n",
        "            single integer, the function will return a single float or a list\n",
        "            depending on whether ``thrs`` is a single float. If the input\n",
        "            ``topk`` is a tuple, the function will return a list of results\n",
        "            of accuracies of each ``topk`` number. That is to say, as long as\n",
        "            ``topk`` is a tuple, the returned list shall be of the same length\n",
        "            as topk.\n",
        "    \"\"\"\n",
        "    assert isinstance(topk, (int, tuple))\n",
        "    if isinstance(topk, int):\n",
        "        topk = (topk, )\n",
        "        return_single = True\n",
        "    else:\n",
        "        return_single = False\n",
        "\n",
        "    if isinstance(pred, torch.Tensor) and isinstance(target, torch.Tensor):\n",
        "        res = accuracy_torch(pred, target, topk, thrs)\n",
        "    elif isinstance(pred, np.ndarray) and isinstance(target, np.ndarray):\n",
        "        res = accuracy_numpy(pred, target, topk, thrs)\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f'pred and target should both be torch.Tensor or np.ndarray, '\n",
        "            f'but got {type(pred)} and {type(target)}.')\n",
        "\n",
        "    return res[0] if return_single else res\n",
        "\n",
        "\n",
        "class Accuracy(nn.Module):\n",
        "\n",
        "    def __init__(self, topk=(1, )):\n",
        "        \"\"\"Module to calculate the accuracy\n",
        "\n",
        "        Args:\n",
        "            topk (tuple): The criterion used to calculate the\n",
        "                accuracy. Defaults to (1,).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.topk = topk\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        \"\"\"Forward function to calculate accuracy\n",
        "\n",
        "        Args:\n",
        "            pred (torch.Tensor): Prediction of models.\n",
        "            target (torch.Tensor): Target for each prediction.\n",
        "\n",
        "        Returns:\n",
        "            list[float]: The accuracies under different topk criterions.\n",
        "        \"\"\"\n",
        "        return accuracy(pred, target, self.topk)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XphaV2TQqiKj",
        "cellView": "form"
      },
      "source": [
        "#@title train (resnest50.py)\n",
        "%cd /content/mmclassification/\n",
        "!python tools/train.py /content/mmclassification/configs/resnest50.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZKO4kBZTNW7"
      },
      "source": [
        "# Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-iJDNl4huzO",
        "cellView": "form"
      },
      "source": [
        "#@title image_demo.py (printing result instead of plotting)\n",
        "%%writefile /content/mmclassification/demo/image_demo.py\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "from mmcls.apis import inference_model, init_model, show_result_pyplot\n",
        "import cv2\n",
        "\n",
        "def main():\n",
        "    parser = ArgumentParser()\n",
        "    parser.add_argument('img', help='Image file')\n",
        "    parser.add_argument('config', help='Config file')\n",
        "    parser.add_argument('checkpoint', help='Checkpoint file')\n",
        "    parser.add_argument(\n",
        "        '--device', default='cuda:0', help='Device used for inference')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # build the model from a config file and a checkpoint file\n",
        "    model = init_model(args.config, args.checkpoint, device=args.device)\n",
        "    # test a single image\n",
        "    result = inference_model(model, args.img)\n",
        "    # show the results\n",
        "    #show_result_pyplot(model, args.img, result)\n",
        "    print(\"result\")\n",
        "    print(result)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RK0tUFZTOBc",
        "cellView": "form"
      },
      "source": [
        "#@title print classification result\n",
        "%cd /content/mmclassification\n",
        "!python demo/image_demo.py /content/image.png \\\n",
        "    /content/mmclassification/configs/resnest50.py \\\n",
        "    /content/mmclassification/work_dirs/resnest50/latest.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFpaa3-sgCU2",
        "cellView": "form"
      },
      "source": [
        "#@title getting topk metrics\n",
        "!python tools/test.py /content/mmclassification/configs/resnest50.py \\\n",
        "    /content/mmclassification/work_dirs/resnest50/latest.pth"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
